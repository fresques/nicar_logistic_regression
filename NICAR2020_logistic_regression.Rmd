---
title: 'NICAR 2020 Stats III: Logistic Regression'
author: "John Perry, Hannah Fresques"
output:
  html_document:
    df_print: paged
---
John Perry
Data Journalism Team Technical Director
The Atlanta Journal-Constitution
John.Perry@ajc.com

Hannah Fresques
Senior Data Reporter
ProPublica
hannah.fresques@propublica.org

[Github repository](https://github.com/fresques/nicar_logistic_regression)

With linear regression, we can learn how explanatory variables influence a continuous normally-distributed result variable. But if the result variable is binary -- yes or no, male or female, passed or failed -- then logistic regression is the tool of choice.

Stories that used logistic regression:

* ["Kept Out"](https://www.revealnews.org/article/how-we-identified-lending-disparities-in-federal-mortgage-data/): Reveal, 2018 
  * investigation of discrimination in mortgage lending 
  * (loan applicants: approved/denied)

* "A Process of Jury Elimination": Dallas Morning News 
  * investigation of discrimination in jury selection 
  * jury pool: excluded/not excluded

* ["Presidential Pardons Heavily Favor Whites"](https://www.propublica.org/article/how-propublica-analyzed-pardon-data): ProPublica, 2011
  * investigation of pardons in the Bush White house 
  * pardon requests: approved/denied

* ["Predict-A-Bill"](https://www.ajc.com/news/local-govt--politics/predict-bill-assesses-chances-bills-becoming-law/lCBiX6zRVWKLf23x1z4esI/): AJC 
  * model to predict bill passage 
  * bills: passed/did not pass

* ["Speed Traps: Who Gets a Ticket, Who Gets a Break?"](https://www.bostonglobe.com/metro/2017/07/28/old-axiom-proves-true-stay-less-than-above-speed-limit-and-you-unlikely-ticketed/HTU3lmnzuQ2hlaG1rvILFN/story.html): Boston Globe, 2017 
  * investigation of discrimination in ticketing for speeding 
  * people stopped for speeding: ticket/warning


## Importing and cleaning data


```{r setup, message = FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
```

```{r, warning=FALSE, message=FALSE}
data <- read_csv('data/boston.csv')
```

This data comes from Boston-area traffic stops. Each case in the data is someone stopped for speeding. The question is, did they get a ticket or just a warning? You can look at the data by clicking the variable name, data, in the RStudio Environment window on your right. We can also get a summary of our data with the str() function.
```{r}
str(data)
```

The data is a collection of character and number fields. Some fields - agency, citation, description - are not useful for our purpose. the useful character fields have already been converted for us to binary 1/0 values. So for convenience sake, we'll eliminate fields that we won't be using.
```{r}
data <- data %>% select(ticket, day, mph, zone, mphover, mphpct, age, minority, female)
head(data)
```

We also want to make sure there's no missing data. The is.na() function returns a TURE/FALSE matrix, TRUE if a value is missing, else FALSE. We can use the apply() function to apply the which function to the TRUE/FALSE matrix (the "2" argument means apply by column) to tell us which columns have missing values in which rows.
```{r}
apply(is.na(data), 2, which)
```

With only one bad row, we can just delete it. More missing data would be a bigger problem.
```{r}
nrow(data)
data <- drop_na(data)
nrow(data)
```

Or all at once with pipes:
```{r message=FALSE, warning=FALSE}

data <- read_csv('data/boston.csv') %>%
  select(ticket,day,mph,zone,mphover,mphpct,age,minority,female) %>%
  drop_na()

summary(data)
```


One more thing to check: is our response variable split roughly equally between yes and no. If one greatly outnumbers the other, then our results will be biased.
```{r}
data %>% count(ticket)
```
Close enough!

## Linear regression with a dummy variable:

With linear regression, you can use a binary explanatory/independent variable. As an example, we can look at how minority --  binary, 0 or 1, variable -- interacts with speed zone. But first, lets do that the most simple way using grouped means:
```{r}
data %>%
  group_by(minority) %>%
  summarise(freq=n(),mean=mean(zone)) %>%
  round(., 2)

```
And we can do essentially the same thing with linear regression:
```{r}
m.binary_independent <- lm(zone ~ minority, data=data)
plot(data$minority, jitter(data$zone))
abline(m.binary_independent, col="red")
```
```{r}
summary(m.binary_independent)
```



## Linear regression and binary result variables?

We know that we can use a binary independent variable in a linear regression model -- this is called a dummy variable. But what if our result variable is binary?
```{r}
plot(jitter(data$mphpct), data$ticket)
```

```{r}
m.linear <- lm(ticket ~ mphpct, data=data)
summary(m.linear)
```

```{r}
plot(jitter(data$mphpct), data$ticket)
abline(m.linear, col='green')
```

Linear regression function gave us a result, but what does it mean?  Values on the regression line represent the best estimate of the mean y value for a certain x value: mean(y) = sum of all the y's/count of cases for a specific x. Since y is binary, either 1 or 0, then that translates to tickets / stops, which is how we define probability. So the predicted value from our linear regression predicted value is the probability of getting a ticket when driving x percent over the speed limit.

## But there's a problem
```{r}
print(m.linear)
```
if we plug 71 percent -- our largest mphpct value -- into the linear equation taken from our regression coefficients, then the probability of getting a tick for driving 71 percent above the speed limit is -0.326162 + 0.019792 * 71
```{r}
-0.326162 + 0.019792 * 71
```

Probabilities above 1 are impossible. A linear function - a function that gives us impossible values - won't work. We need a function that only gives us values between 1 and 0:
```{r}
# recreate our previous plot
plot(data$mphpct, data$ticket, xlab="mphpct", ylab="ticket probability")
m.linear <- lm(ticket ~ mphpct, data=data)
abline(m.linear, col='green')

# add logit function plot

# create a logistic regression model:
m.logistic <- glm(ticket ~ mphpct, data=data, family='binomial')

# create a list of 100 mphpct values between 0 and 5 more than our highest value:
plotData <- data.frame(mphpct=seq(0,max(data$mphpct) + 5, len=100))

# Us our model to add the predicted probability of getting a ticket for those mphpct values:
plotData$ticket = predict(m.logistic, newdata=plotData, type='response')

# plot it:
lines(ticket~mphpct, plotData, col='red')
```


But instead of $p(y)=\beta _{0}+\beta _{1}x$, we now have: 

$$p(y)={\frac {e^{(\beta _{0}+\beta _{1}x)}}{e^{(\beta _{0}+\beta _{1}x)}+1}}$$

How do we describe this relationship in our story?

## A digression: Probability and Odds

* Probability = tickets / all traffic stops
* Odds = tickets / warning
  + Also: p(tickets) / p(warning)
  + Also: p(tickets) / 1 - p(tickets)

* Probability of pulling a diamond from a deck of cards:
  + 13 / 52 = 0.25 (25%)
* Odds of pulling a diamond from a deck of cards:
  + 13 / 39 = 1/3 = 0.33
* or as a probability ratio:
  + 0.25 / (1 - 0.25) = 0.33

O = p / (1-p)

p = O / (O + 1)

Probability can be any value from 0 to 1. Which is why we can't use linear regression to predict it!

Odds can be any value from 0 and infinity. If the odds are in your favor - greater than 1:1 - the odds are between 1 and infinity. If the odds are against you - less than 1:1 - then the odds are between 0 and 1.

Odds aren't a good dependent variable for linear regression either, because predicted values from linear regression can be -infinity to +infinity.

BUT! log(Odds) gives you any values from -infinity to +infinity. Some examples:

log(3 to 1) = log(3) = 1.1 

log(1 to 3) = log(0.3333) = -1.1


## Log(Odds): the Logit Function

Our weird friend from before:

$$p(y)={\frac {e^{(\beta _{0}+\beta _{1}x)}}{e^{(\beta _{0}+\beta _{1}x)}+1}}$$

Can also be written as:
(do the algebra if you want to check)

$${\frac {p(y)}{1-p(y)}}={e^{(\beta _{0}+\beta _{1}x)}}$$

which is the same as 

$$Odds(y)={e^{(\beta _{0}+\beta _{1}x)}}$$

If we take the log of both sides, we get:

$$log(Odds(y))=\beta _{0}+\beta _{1}x$$
This is the logit function. TADA.

If we convert the probabilities of getting a ticket to the log of the odds of getting a ticket, we're back to a line:
```{r}
# add log odds values to our plot data
# (remember that on plotData, ticket is continuous 0-1)
plotData <- plotData %>% 
  mutate(
    ticketLogOdds = log(ticket / (1 - ticket))
    )

# plot the log odds of getting a ticket as predicted by mphpct
plot(plotData$mphpct, plotData$ticketLogOdds, type="l", col="red", ylab="log(odds of getting a ticket)", xlab="mphpct")
```

A Generalized Linear Model (GLM) is $linkFunction(y)=\beta _{0}+\beta _{1}x$

In logistic regression, the link function is $log({\frac {p}{1-p}})$, or $log(Odds)$

## Analyzing the data ... finally

Manually calculating the probability and odds of getting a ticket, or not.
```{r}
# (remember that on data, ticket is binary 0/1)
data %>%
  group_by(ticket) %>%
  summarise(
    freq=n(),
    prob=(n()/nrow(.)),
    odds=prob/(1-prob), 
    logodds=log(odds)
  ) %>%
  round(., 4)
```

## The Null Model: should match simple calculations above
```{r}
m.null <- glm(ticket ~ 1, data=data, family='binomial')
summary(m.null)
```
Exponent of the coefficient
```{r}
exp(coef(m.null))
```

## A more interesting model: ticket ~ mphpct
```{r}
m.mphpct <- glm(ticket ~ mphpct, data=data, family='binomial')
summary(m.mphpct)
```

The coefficient for mphpct is 0.09205, which tells us (just like in linear regression) that a 1 percentage point increase in mphpct means an increase in the log of the odds of getting a ticket of about 0.09:

log odds at (mphpct + 1) = (log odds at mphpct) + 0.09

If you want to put this relationship in terms readers will understand, like odds, we take the exponent of the coefficient. But by taking the exponent, we change the addition to multiplication:
```{r}
exp(coef(m.mphpct))
```

odds at (mphpct + 1) = (odds at mphpct) * 1.09

Or

For every one percentage point increase in speed over the limit, you increase your odds of getting a ticket by 9 percent.

## What if the explanatory/independent variable is binary?
```{r}
m.minority <- glm(ticket ~ minority, data=data, family='binomial')
summary(m.minority)
```
Intercept represents the estimate for non-minorities. The minority coefficient is the log of the odds ratio: minority odds / non-minority odds.
```{r}
exp(coef(m.minority))
```
Odds of a minority driver getting a ticket are 8-times greater than for a white driver.

```{r}
# (remember that on data, ticket is binary 0/1)
data %>%
  group_by(minority) %>%
  summarise(
    n=n(),
    n_tickets=sum(ticket),
    prob=n_tickets/n,
    odds=prob/(1-prob), 
    logodds=log(odds)
  ) %>%
  round(., 4)
```

The odds ratio is 5/0.5965 = 8.38223, which matches the coefficent above. 


## R-squared ... sorta
McFadden's Pseudo R-squared
```{r}
ll.null <- m.minority$null.deviance / -2
ll.proposed <- m.minority$deviance / -2
(ll.null - ll.proposed) / ll.null
```
p-value for R-squared:
```{r}
1 - pchisq(2*(ll.proposed - ll.null), df=(length(m.minority$coefficients) - 1))
```

## Let's make it multivariate
```{r}
m.best_predictions <- glm(ticket ~ minority + mphover + female + age, data=data, family=binomial)
summary(m.best_predictions)
```
McFadden's Pseudo R-squared
```{r}
ll.null <- m.best_predictions$null.deviance/-2
ll.proposed <- m.best_predictions$deviance/-2
(ll.null - ll.proposed) / ll.null
```
p-value for R-squared:
```{r}
1 - pchisq(2*(ll.proposed - ll.null), df=(length(m.best_predictions$coefficients) - 1))
```
Visualize our predictions:
```{r}
predicted.data <- data.frame(
  probability = m.best_predictions$fitted.values,
  ticket=data$ticket
  ) %>% 
  mutate(
    rank=rank(probability),
    ticket_desc=ifelse(ticket==1,'yes','no')
  )

ggplot(data=predicted.data, aes(x=rank, y=probability)) +
  geom_point(aes(color=ticket_desc), alpha=0.8, shape=4, stroke=2) +
  scale_color_discrete(name="Actually got a ticket?") +
  scale_y_continuous(labels = scales::percent) +
  xlab("Index") +
  ylab("Predicted probability of getting a ticket") +
  theme_minimal()
```



